{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "from copy import deepcopy as dp\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from model_functions import Model, TrainDataset, TestDataset\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n",
    "\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
    "    ss_1_dic = {'zsco':StandardScaler(),\n",
    "                'mima':MinMaxScaler(),\n",
    "                'maxb':MaxAbsScaler(), \n",
    "                'robu':RobustScaler(),\n",
    "                'norm':Normalizer(), \n",
    "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
    "                'powe':PowerTransformer()}\n",
    "    ss_1 = ss_1_dic[sc_name]\n",
    "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    if saveM == False:\n",
    "        return(df_2)\n",
    "    else:\n",
    "        return(df_2,ss_1)\n",
    "\n",
    "def norm_tra(df_1,ss_x):\n",
    "    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    return(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [86]\n",
    "\n",
    "sc_dic = {}\n",
    "feat_dic = {}\n",
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_targets_scored = pd.read_csv('train_targets_scored.csv')\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "train_drug = pd.read_csv('train_drug.csv')\n",
    "\n",
    "target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feat_dic['gene'] = GENES\n",
    "feat_dic['cell'] = CELLS\n",
    "\n",
    "# sample norm \n",
    "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
    "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "# remove ctl\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[['sig_id']+target_cols]\n",
    "\n",
    "train0 = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "# drug ids\n",
    "tar_sig = target['sig_id'].tolist()\n",
    "train_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\n",
    "target = target.merge(train_drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_drug.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 19].index\n",
    "vc2 = vc.loc[vc > 19].index\n",
    "\n",
    "feature_cols = []\n",
    "for key_i in feat_dic.keys():\n",
    "    value_i = feat_dic[key_i]\n",
    "    print(key_i,len(value_i))\n",
    "    feature_cols += value_i\n",
    "len(feature_cols)\n",
    "feature_cols0 = dp(feature_cols)\n",
    "    \n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, train0.shape)    # train0 doesnt have cp_type\n",
    "print(test.shape)\n",
    "print(target.shape, np.shape(target_cols))      # Target columns names to a list (2 less since sig_id and index)\n",
    "print(vc1.shape, vc2.shape)     #vc1 drugs less than 19 appearances and vc2 more than 19 appearances\n",
    "print(len(feature_cols0))\n",
    "print(len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "for seed in SEED:\n",
    "\n",
    "    seed_everything(seed=seed)\n",
    "    folds = train0.copy()\n",
    "    feature_cols = dp(feature_cols0)\n",
    "    \n",
    "    # kfold - leave drug out\n",
    "    target2 = target.copy()\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 19X\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    target2['kfold'] = target2.drug_id.map(dct1)\n",
    "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
    "    target2.kfold = target2.kfold.astype(int)\n",
    "\n",
    "    folds['kfold'] = target2['kfold'].copy()\n",
    "\n",
    "    train = folds.copy()\n",
    "    test_ = test.copy()\n",
    "\n",
    "    # HyperParameters\n",
    "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    EPOCHS = 3\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    NFOLDS = 5\n",
    "    EARLY_STOPPING_STEPS = 10\n",
    "    EARLY_STOP = False\n",
    "\n",
    "    n_comp1 = 50\n",
    "    n_comp2 = 15\n",
    "\n",
    "    num_features=len(feature_cols) + n_comp1 + n_comp2\n",
    "    num_targets=len(target_cols)\n",
    "    num_targets_0=num_targets\n",
    "    hidden_size=4096\n",
    "\n",
    "    #tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
    "    #tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "    #tar_weight0_min = dp(np.min(tar_weight0))\n",
    "    #tar_weight = tar_weight0_min/tar_weight0\n",
    "    #pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
    "\n",
    "    def run_training(fold, seed):\n",
    "\n",
    "        seed_everything(seed)\n",
    "\n",
    "        trn_idx = train[train['kfold'] != fold].index\n",
    "        val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
    "        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
    "\n",
    "        x_train, y_train = train_df[feature_cols], train_df[target_cols].values\n",
    "        x_valid, y_valid = valid_df[feature_cols], valid_df[target_cols].values\n",
    "        x_test = test_[feature_cols]\n",
    "\n",
    "        #------------ norm --------------\n",
    "        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
    "        col_num.sort()\n",
    "        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
    "        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
    "        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
    "\n",
    "        #------------ pca --------------\n",
    "        def pca_pre(tr,va,te,\n",
    "                    n_comp,feat_raw,feat_new):\n",
    "            pca = PCA(n_components=n_comp, random_state=42)\n",
    "            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
    "            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
    "            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
    "            return(tr2,va2,te2)\n",
    "\n",
    "\n",
    "        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
    "        feat_dic['pca_g'] = pca_feat_g\n",
    "        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
    "\n",
    "        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
    "        feat_dic['pca_c'] = pca_feat_g\n",
    "        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
    "\n",
    "        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
    "\n",
    "        model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets_0,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        train_dataset = TrainDataset(x_train, y_train)\n",
    "        valid_dataset = TrainDataset(x_valid, y_valid)\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "        loss_tr = nn.BCEWithLogitsLoss()\n",
    "        loss_va = nn.BCEWithLogitsLoss()    \n",
    "\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        best_loss = np.inf\n",
    "\n",
    "        mod_name = f\"FOLD_mod11_{seed}_{fold}_.pth\"\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "\n",
    "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
    "            print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "\n",
    "                best_loss = valid_loss\n",
    "                oof[val_idx] = valid_preds\n",
    "                torch.save(model.state_dict(), mod_name)\n",
    "\n",
    "            elif(EARLY_STOP == True):\n",
    "\n",
    "                early_step += 1\n",
    "                if (early_step >= early_stopping_steps):\n",
    "                    break\n",
    "\n",
    "        #--------------------- PREDICTION---------------------\n",
    "        testdataset = TestDataset(x_test)\n",
    "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(torch.load(mod_name))\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        predictions = np.zeros((len(test_), len(target_cols)))\n",
    "        predictions = inference_fn(model, testloader, DEVICE)\n",
    "        return oof, predictions\n",
    "\n",
    "    def run_k_fold(NFOLDS, seed):\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "        for fold in range(NFOLDS):\n",
    "            oof_, pred_ = run_training(fold, seed)\n",
    "\n",
    "            predictions += pred_ / NFOLDS\n",
    "            oof += oof_\n",
    "\n",
    "        return oof, predictions\n",
    "\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    oof_tmp = dp(oof)\n",
    "    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
    "    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n",
    "    \n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
    "\n",
    "train0[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "### for blend test ###\n",
    "train0.to_csv('train_pred.csv', index=False)\n",
    "### for blend test ###\n",
    "\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(train[target_cols].values, oof.round()))\n",
    "print(\"ACCURACY OF THE MODEL: \", np.mean([accuracy_score(train[target_cols].iloc[:,i].values,oof[:,i].round()) for i in range(len(target_cols))]))\n",
    "print(\"ACCURACY OF THE MODEL: \", np.mean([accuracy_score(train[target_cols].iloc[i,:].values,oof[i,:].round()) for i in range(len(target_cols))]))\n",
    "print(\"Log loss: \", np.mean([log_loss(train[target_cols].iloc[:,i].values,oof[:,i]) for i in range(len(target_cols))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oof[0,:].round().shape)\n",
    "print(train[target_cols].iloc[0,:].shape)\n",
    "train[target_cols].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
